# LIME config file, generated with lime init command
# template lime v0.1 config:

# Do NOT store secrets in here. 
# Instead use usr config dir: ~/.lime/secrets.env
# these will be loaded to common.models.state.Secrets
# =============

# Custom Models will be utilized when specified from the -m flag on eval
# command of specfied in ExecSettings.default_model. This will 
LocalModels:
  # Example of Local model where we specify (usually absolute path)
  # to the weights file to load into llama_cpp engine.
  # No need to set type, as it defaults to `local`.
  my_llama:
    fn: 'path/to/weights'
  # Example of specifying a CPL model, done by setting type to `cpl`.
  # We can also use a `profile` here to add and/or override params:
  # (like with rag_style) wh with and/or override
  my_cpl_rag1:
    type: cpl
    profile:
      rag_style: basic
      max_tokens: 20
  # Example of OpenAI custom model which do by specifying type as `openai` 
  # and also giving a valid `api_model_name`.
  secret_weapon:
    type: openai
    api_model_name: 'gpt-4'

# LocalParams specify generation parameters used in all model / inference types. 
# The values below are the default values.
LocalParams:
  temperature: 0.0
  max_tokens: 100
  seed: null

# These Init Params apply to the LocalModels and are passed into llama_cpp model
# constructors.
LocalModelInitParams:
  n_ctx: 512
  n_threads: 4

# These are the main settings for running eval command
ExecSettings:
  # What level of verbosity that eval (and other commands) use.
  # 0: silent, 1: some verbosity, 2: full verbosity
  verbose: 0
  # How many random chars to add at the end of an output file.
  uuid_digits: 4
  # Filename prefix used for finding valid input-sheets in a directory.
  # Set to empty string to collect all files.
  input_sheet_prefix: 'input'
  # Filename prefix used for finding valid output jsons
  output_sheet_prefix: 'output'
  # Default model_name used when not running eval command without -m arg.
  model_name: 'gpt-3.5-turbo'
  # Set to true, to write after each question to tmp-{outputfn}.json
  # Useful for saving progress over long runs
  save_tmp_file: False
  # When using LocalModels
  use_prompt_cache: False    # Maybe move to init params?

# These settings govern the hosting options of the CplServer, and are also
# used by the CplClient to point its requests
CplServerConfig:
  # whether the server is looking for ssl
  is_https: false
  # default domain 
  domain: localhost
  # port to run flask on
  port: 5000
  # if the flask server should run in debug mode or not
  debug: True
  # configuration option for the url on different actions
  endpoint_check: check
  endpoint_infer: infer

# These settings are used for report collection and display, the `agg` command.
# These settings override the settings applied when running --md setting, or
# automatically when you pipe the output to a file. There most effect the 
# reformatting of the potentially long text of `completion`
AggSettings:
  # how many characters wide the output should be; broken up by line breaks
  max_width: 60
  # max number of lines to display
  max_height: 7
  # max number of characters to display; all further characters truncated
  max_chars: 300
  # set to true, converts line carriages (`\n`) to html line breaks (`<br>`)
  replaces_br: True
  # set to true, turns off all the above re-formatting options
  no_format: False


# These parameters impact how the client communicates with CPL server
CplClientParams:
  # by default this is set to null, meaning it won't add anything other
  # than the prompt in the payload of the request from client to server...
  valid_request_args: null
  # ...however we can also make this a list of strings 
  # e.g. ['rag_style', 'temperature'], and these parameters will be added
  # in the payload of the request as the key names, and their respective values.
  # see cpl_client.CPLModelObj.valid_request_args. For example:
  # valid_request_args:
  # - rag_style
  # - temperature
  # an example of specifying an argument which the cpl client will utilize.
  rag_style: colbert

# These parameters allow us to modify default behavior when CPL server.
# is spun up.
CplServerParams:
  # this is simply an example parameter that server might use.
  k_retrieved: 3
  # similiar to CplClientParams.valid_request_args, but the server will
  # respond with a 400 if a request does not contain all parameters specified
  # within it. By default it's set to None, so server only requires the 
  # `question` parameter in the request payload
  required_infer_keys: null

# end config